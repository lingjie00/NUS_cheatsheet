\section{Probability Theory}

\subsection{Measure Space $(\Omega, \mathcal{F}, \nu)$ and Measurable functions}

\subsubsection{measure spaces}
$\Omega$ := sample space, $(\Omega, \mathcal{F})$ := measurable space, $A_i\in\mathcal{F}$ := $\mathcal{F}$-measurable,
$(\Omega, \mathcal{F}, \nu)$ := measure space

if $\nu(\Omega)=1$ := probability space, $\nu(A)=P(A)$ := probability of event $A$

\subsubsection{$\sigma$-field}
A collection $\mathcal{F}$ of subsets of a set $\Omega$ is called a $\sigma$-field if

(1) $\emptyset \in \mathcal{F}$
(2) $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$
(3) $A_i \in \mathcal{F} \Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}$

\subsubsection{smallest $\sigma$-field}
Given collection $\mathcal{C}$, there exists a smallest $\sigma$-field $\mathcal{F}:=\sigma(\mathcal{C})$ s.t.

(1) $\mathcal{C}\subset\mathcal{F}$
(2) if $\mathcal{E}$ is a $\sigma$-field containing $\mathcal{C}$, then $\mathcal{F}\subset\mathcal{E}$

\subsubsection{Borel $\sigma$-field}
$\mathcal{B}$ := $\sigma(\mathcal{O})$ or $\mathcal{B}^d=\sigma(\mathcal{O}^d)$ where $\Omega=\mathcal{R}$, $\mathcal{O}$ is the collection of all open sets

\subsubsection{measure}
A (positive) measure $\nu$ on a measurable space $(\Omega, \mathcal{F})$ is a non-negative function $\nu: \mathcal{F} \rightarrow \mathcal{R}$ s.t.
\newline(1. non-negativity) $0\leq \nu(A) \leq \infty$ $\forall$ $A \in \mathcal{F}$
\newline(2. empty is zero) $\nu(\emptyset)=0$
\newline(3. $\sigma$-additivity) $\sum_{i=1}^\infty \nu(A_i) \ \nu(\cup_{i=1}^\infty) A_i$ if $A_i\in\mathcal{F}$ are disjoint

\subsubsection{common measure}
(counting measure $\nu(A)$) number of elements in $A$ $\forall$ $A \in \Omega$, can be $\infty$

(Lebesgue measure $m([a, b])$) $(b-a) I(a < b)$ default for countable sets in $(\mathcal{R}, \mathcal{B})$, is $\sigma$-finite as $\exists A_i=[-i, i]$

\subsubsection{measure properties}
(1. Monotonicity) $A\subset B \Rightarrow \nu(A) \leq \nu(B)$
\newline(2. Sub-additivity) any sequence of potentially non-disjoint set $A_n$, $\nu(\cup_{n=1}^\infty A_n) \leq \sum_{n=1}^\infty \nu(A_n)$
\newline(3. Continuity of Increasing sequences) $\lim_{n\rightarrow\infty} A_n := \cup_{n=1}^\infty A_n$ and $\nu(\lim_{n\rightarrow \infty} A_n) = \lim_{n\rightarrow \infty} \nu(A_n)$
\newline(4. Continuity of Decreasing sequences) $\lim_{n\rightarrow \infty} A_n := \cap_{n=1}^\infty A_n$, and if $\nu(A_1) < \infty$ then $\nu(\lim_{n\rightarrow \infty} A_n) = \lim_{n\rightarrow \infty} \nu(A_n)$

\subsubsection{$\sigma$-finite}
measure is $\sigma$-finite if $\exists$ a seq of measurable sets $A_i$ s.t. $\cup_{i=1}^\infty A_i = \Omega$ and $\nu(A_i)<\infty$  $\forall$ $i$

\subsubsection{product}
$\sigma$-field $\prod_{i=1}^d \mathcal{F}_i$] $(\Omega_1\times \Omega_2 \times \cdots \Omega_d, A_1\times \cdots \times A_d: A_i \in \mathcal{F}_i)$

\subsubsection{product measure} Suppose $(\Omega_i, \mathcal{F}_i, \nu_i)$ are measure spaces and $\nu_i$ are all $\sigma$-finite. There exists a unique $\sigma$-field measure on product $\sigma$-field s.t. $\forall A_i\in\mathcal{F}_i$
$$
\nu_i\times\cdots\times\nu_d(A_1\times\cdots\times A_d) = \prod_{i=1}^d\nu_i(A_i)
$$

\subsubsection{measurable function}
Let $(\Omega, \mathcal{F}), (\Lambda, \mathcal{G})$ be measurable spaces and $f: \Omega \rightarrow \Lambda$ be a function. The function $f$ is measurable function from $(\Omega, \mathcal{F})$ to $(\Lambda, \mathcal{G})$ if
$$
f^{-1}(A) \in \mathcal{F} \text{ for all } A \in \mathcal{G}
$$

Note $f^{-1}(\mathcal{G})=\sigma(f)=\{f^{-1}(A):A\in\mathcal{G}\}$ is a sub-$\sigma$-field to $\mathcal{F}$
\newline(Richness) addition, multiplication, division, composition, sup, limit preserve measurability

\subsubsection{simple function}
with a finite $k$, $c_i \geq 0, A_i \in \mathcal{F}$
$$f(\omega):=\sum_{i=1}^k c_iI_{A_i}(\omega)$$ 

\subsubsection{approximation by simple function}
Any non-negative Borel function $f$ can be approximated by s sequence of increasing simple functions $\varphi_n$ and $\lim_n \varphi_n (x) = f(x)$ for every $x\in\Omega$
$$
\varphi_n=\sum_{i=0}^{n2^n-1} \frac{i}{2^n}I\left(\frac{i}{2^n}\leq f \leq \frac{i+1}{2^n}\right) + n I(f \geq n) ~\forall~ n \in \mathcal{N}
$$

\subsubsection{measurable function in probability}

(random element) denoted as $X$, (random variable) if $X$ is real valued, (random vector) if $X$ is vector-valued

\subsubsection{a.e., a.s., w.p}

$A$ is null set if $\nu(A) = 0$
(almost everywhere) statement holds $\nu$-a.e. if statement holds for all $\omega \in A^c$
(almost surely) if $\nu$ is a probability
(with probability 1) alternatively, we say the statement holds w.p. 1

\subsection{Integration and Expectation}

[Integral for simple function] $\infty$ is allowed and $c\times \infty=\infty$ if $c>0$ else $0$
$$\int f d\nu := \sum_{i=1}^k c_i \nu(A_i)$$

\subsubsection{Integral for non-negative Borel functions}
$\mathcal{S}_f$ := collection of all non-negative simple functions s.t. $g\leq f$ if $g\in\mathcal{S}_f$
$$\int f d\nu := \sup \left\{ \int f d\nu: g\in \mathcal{S}_f \right\} = \lim_n \int \varphi_n d\nu$$
if $0\leq \varphi_0 \leq \varphi_1 \leq \cdots \leq f$ and $\lim_n \varphi_n = f$

\subsubsection{Integral for arbitrary Borel functions}
$f = f_+ - f_-$, $f_+ = \max\{f(x), 0\}$, $f_- = \max\{ -f(x), 0 \}$

$$\int f d\nu := \int f_+ d\nu - \int f_- d\nu$$
when both $f_+, f_-$ are finite, $f$ is integrable

\subsubsection{integral over subset, and notation}
$I_A$ is measurable, so is the product $I_A f$. If it exist, then $\int_A f d\nu = \int I_A f d\nu$

$$\int f d\nu = \int_\Omega f d\nu = \int f(x) d\nu(x) = \int f(x) \nu (dx)$$

\subsubsection{Expectation}

for a probability measure $P$
$$EX=E(X)=\int X dP$$

\subsubsection{Expectation properties}
(Linearity) $E(aX + bY) = aEX + bEY$
\newline (Absolute finite) $EX$ is finite if and only if $E|X|$ is finite
\newline(Order) if $X\geq 0$ a.s. then $EX\geq 0$, if $X\leq Y$ a.s. then $EX \leq EY$, if $X=Y$ a.s. then $EX=EY$
\newline (Absolute order) $|EX|\leq E|X|$
\newline (Deduce $X=0$) If $X\geq 0$ a.s. and $EX=0$ then $X=0$ a.s.

\subsection{Convergence Theorem}
Let $\left\{ f_n \right\}_{n=1}^\infty$ be a sequence of Borel functions on $(\Omega, \mathcal{F}, \nu)$

\subsubsection{Monotone convergence theorem}

if $0\leq f_1 \leq f_2 \leq \cdots$ and $\lim_n f_n = f$ a.e. then
$$\int \lim_n f_n d\nu = \lim_n \int f_n d\nu$$

\subsubsection{Fatou's lemma} If $f_n \geq 0$
$$\int \lim\inf_n f_n d\nu \leq \lim\inf_n \int f_n d\nu$$

\subsubsection{Dominated convergence theorem}
If $\lim_{n\rightarrow \infty} f_n = f $ and $\exists $ integrable function $g$ s.t. $|f_n|\leq g$ a.e.
$$\int\lim_n f_n d\nu = \lim_n \int f_n d\nu$$

\subsection{Change of variables}

\subsubsection{Interchange differentiation and Integration}
(1) Suppose $\exists$ $(a, b)\subset \mathcal{R}$ which $\partial f(\omega,\theta)/\partial\theta$ exists a.e. (2) There is an integrable function $g$ on $\omega$ s.t. $|\partial f(\omega,\theta/\partial\theta|\leq g(\omega)$ a.e.

$$
\frac{d}{d\theta}\int  f(\omega,\theta) d\nu(\omega) = \int \frac{\partial f(\omega, \theta)}{\partial\theta} d\nu(\omega)
$$

\subsubsection{Change of variable}
Let $(\Omega, \mathcal{F}, \nu)$ be measure space, $(\Lambda, \mathcal{G})$ be measurable space, $f$ is measurable from $(\Omega, \mathcal{F})$ to $(\Lambda, \mathcal{G})$, 
$f$ induce a measure $\nu \circ f^{-1}(B) := \nu(f^{-1}(B))\in\Lambda$ $\forall$ $B\in\mathcal{G}$.
Suppose $g$ is Borel function on $(\Lambda, \mathcal{G})$,
$$
\int_\Omega g \circ f d\nu = \int_\Lambda g d (\nu \circ f^{-1})
$$

\subsubsection{Change of Var Formula}

$Y=g(X)$, $A_i$ disjoint, $h_j$ is inverse function of $g$ on $A_j$.

$$f_Y(y)=\sum_{j:1\leq j \leq m, y\in g(A_j)}\left|\det\left(\frac{\partial h_j(y)}{\partial y}\right)\right| f_X(h_j(y))$$

\subsection{Cumulative Distribution Function}

\subsubsection{Law of $X$ (or the distribution of $X$)} when $\nu = P$, $(\Lambda, \mathcal{G}) = (\mathcal{R}, \mathcal{B})$ and $f=X$ (random variable).
Then $P\circ X^{-1}$ if often denoted by $P_X$ or $F_X$ (CDF). Where $F_X(c) = P(X\leq c)$ By change of variable

$$
E g(X) = \int_\Omega g(X(\omega)) dP(\omega) = \int_\mathcal{R} g(x) dP_X(x)  = \int_\mathcal{R} g(x) dF_X(x)
$$

\subsubsection{CDF properties}

$F(-\infty) = \lim_{x\rightarrow -\infty}F(x) = 0; F(\infty)  = \lim_{x\rightarrow \infty}F(x) = 1$

$F$ is non-decreasing. $F(x) \leq F(y)$ if $x \leq y$

$F$ is right-continuous. $\lim_{y\rightarrow x + 0} F(y) = F(x)$

\subsection{Fubini's Theorem}

Suppose $f\geq 0$ or $\int |f|d(\nu_1 \times \nu_2) < \infty$ then

$$
g(\omega_2) = \int_{\Omega_1} f(\omega_1, \omega_2) d\nu_1(\omega_1)
$$

$$
\int_{\Omega_1 \times \Omega_2} f d(\nu_1 \times \nu_2) = \int_{\Omega_1}\left[ \int_{\Omega_2} f(\omega_1, \omega_2) d\nu_1(\omega_1) \right] d\nu_2(\omega_2)
$$

\subsection{Radon-Nikodym derivatives}

\subsubsection{Absolutely continuity}
$\lambda << \nu$ iff for any $A \in \mathcal{F}$, $\nu(A) = 0 \Rightarrow \lambda(A) = 0$

\subsubsection{Radon-Nikodym}

$\lambda << \nu$, there exist unique $f$ s.t.

$$
\lambda(A) = \int_A f d\nu, A\in \mathcal{F}
$$

\subsubsection{PDF}

Given probability measure $P$ and $\sigma$-finite $\nu$, if $P << \nu$ then $\frac{dP}{d\nu}$ is called the pdf of $P$ w.r.t. $\nu$

\subsubsection{Lebesgue PDF}

When $P << m$ (Lebesgue measure), $\frac{dP}{dm}$ is called Lebesgue PDF of $P$ (or of $F$). If $F$ has derivative $f$, then

$$
P((-\infty, x]) = F(x) = \int_{-\infty}^x f(y) dm(y) = \int_{-\infty}^x f(y) dy
$$

\subsubsection{Calculus with Radon-Nikodym derivatives}
If $\lambda << \nu$ and $f \geq 0$

$$
\int f d\lambda = \int f \frac{d\lambda}{d\nu} d\nu
$$

If $\lambda_i << \nu$, then $\lambda_1 + \lambda_2 << \nu$

$$
\frac{d(\lambda_1 + \lambda_2)}{d\nu} = \frac{d\lambda_1}{d\nu} + \frac{d\lambda_2}{d\nu}
$$

If $\lambda$ is $\sigma$-finite, and $\tau << \lambda << \nu$, then $\nu$-a.e.

$$
\frac{d\tau}{d\nu} = \frac{d\tau}{d\lambda}\frac{d\lambda}{d\nu}
$$

If $\lambda << \nu$ and $\nu << \lambda$, then with $\nu$ or $\lambda$-a.e.

$$
\frac{d\lambda}{d\nu} = \left(\frac{d\nu}{d\lambda}\right)^{-1}
$$

If $\nu_i$ is $\sigma$-finite, $\lambda_i << \nu_i$ then $\lambda_1 \times \lambda_2 << \nu_1 \times \nu_2$, then for $(\nu_1\times \nu_2)$-a.e.

$$
\frac{d(\lambda_1 \times \lambda_2)}{d(\nu_1\times \nu_2)} (\omega_1, \omega_2) = \frac{d\lambda_1}{d\nu_1}(\omega_1)\cdot \frac{d\lambda_2}{d\nu_2} (\omega_2)
$$

\subsection{Moments}
[$p$th absolute moment]
$E[|X|^p]$
\
[$k$th moment]
$E[X^k]$
\
[$k$th central moment]
$E[(X-\mu)^k]$

\subsubsection{Variance, Covariance}

$Var(X) = E[(X-EX)(X-EX)^T]$

$Cov(X, Y) = E[(X-EX)(Y-EY)^T]$

$Corr(X, Y) = Cov(X, Y)/(\sigma_X\sigma_Y)$

$E(a^TX)=a^TEX$

$Var(a^TX)=a^TVar(X)a$

\subsection{Probability Inequalities}

\subsubsection{Cauchy-Schewarz inequality}
$$
Cov(X, Y)^2 \leq Var(X)Var(Y)
$$
$$
(EXY)^2 \leq EX^2 EY^2
$$

\subsubsection{Jensen's inequality}

$A$ is a convex set in $\mathcal{R}^d$, $\varphi$ is a convex function on $A$ and $X\in A$ is a $d$-random vector
$$
\varphi(EX) \leq E\varphi(X)
$$
If $\varphi$ is strictly convex and $\varphi(X)$ is not a constant, then $\varphi(EX) < E\varphi(X)$

$$(EX)^{-1} < E(X^{-1})$$
$$E(logX)<log(EX)$$
$$
\int f\log\left(\frac{f}{g}\right)d\nu \geq 0
$$

\subsubsection{Chebyshev's inequality}
$X$ is R.V, $\varphi$ is nonnegative and symmetric function ($\varphi(-x) = \varphi(x)$) and is non-decreasing on $[0, \infty)$, then for each constant $t \geq 0$
$$
\varphi(t) P(|X|\geq t) \leq \int_{\{|X|\geq t\}} \varphi(X) dP \leq E\varphi(X)
$$

Common results
$$
P(|X-\mu| \geq t) \leq \frac{\sigma_X^2}{t^2},
P(|X|\geq t) \leq \frac{E|X|}{t}
$$

\subsubsection{Hölder's inequality}

suppose $p, q > 0$ are Hölder's conjugate s.t. $1/p + 1/q = 1\Rightarrow q = p / (p-1)$
$$
E|XY| \leq (E|X|^p)^{1/p}(E|Y|^q)^{1/q}
$$
If both $E|X|^p$ and $E|Y|^q$ are finite, equality holds if and only if $|X|^p$ and $|Y|^q$ are linearly dependent

\subsubsection{Young's inequality} equality if and only if $a^p = b^q$
$$
ab \leq \frac{a^p}{p} + \frac{b^q}{q}
$$

\subsubsection{Minkowski's inequality} $p \geq 1$
$$
(E|X+Y|^p)^{1/p} \leq (E|X|^p)^{1/p} + (E|Y|^p)^{1/p}
$$

\subsubsection{Lyapunov's inequality} for $0 < s < t$
$$
(E|X|^s)^{1/2} \leq (E|X|^t)^{1/t}
$$

\subsubsection{Kullback-Leibler Information}

$K(f_0, f_1) = E_0 \log \frac{f_0(X)}{f_1(X)} = \int \log \left(\frac{f_0(x)}{f_1(x)}\right) f_0(x)d\nu(x) \geq 0$ with equality if and only if $f_1(\omega)=f_0(\omega)$ $\nu$-a.e.

\subsubsection{Shannon-Kolmogorov information equality}

$K(f_0, f_1)\geq 0$ with equality if and only if $f_1(\omega)=f_0(\omega)$ $\nu$-a.e.

\subsection{Characteristic function, moment generating function}
$\forall$ $t\in\mathcal{R}^d$

[Char func] $|\phi_X|\leq 1$, $\phi_{-X}=\overline{\phi_X(t)}$
$$
\phi_X(t) = E\left[exp(\sqrt{-1}t^TX\right]=E\left[\cos(t^TX) + \sqrt{-1}\sin(t^TX)\right]
$$

[MGF] $\psi_{-X}(t) = \psi_{X}(-t)$
$$
\psi_X(t) = E\left[exp(t^TX)\right]
$$

if $\psi$ is finite in neighborhood of $\mathbf{0}\in\mathcal{R}^d$, then moments of $X$ of any order are finite, and $\phi_X(t)=\psi_X(\sqrt{-1}t)$

\subsection{Condition on information}

\subsubsection{Conditional expectation}
[Conditional Expectation]
$E(X|\mathcal{A})$ is random variable satisfying

(1) $E(X|\mathcal{A})$ is measurable from $(\Omega, \mathcal{A})$ to $(\mathcal{R}, \mathcal{B})$

(2) $\int_C E(X|\mathcal{A})dP=\int_C X dP$ for any $C\in \mathcal{A}$.
Such $E(X|\mathcal{A})$ exists and is unique

[Conditional probability]
$P(B|\mathcal{A})=E(I_B|\mathcal{A})$

[Conditional Expectation given $Y$]
$E(X|Y):=E[X|\sigma(Y)]$

\subsubsection{Conditional Expectation given $y$}
Let $Y$ be measurable from $(\Omega, \mathcal{F})$ to $(\Lambda, \mathcal{G})$ and $Z$ a function from $(\Omega, \mathcal{F})$ to $\mathcal{R}^k$. If $Z$ is Borel on $(\Omega, \sigma(Y))$, then there is a Borel function $h$ on $(\Lambda, \mathcal{G})$ such that $Z=h\circ Y$. We can denote $h$ as $E(X|Y=y)$

\subsubsection{Simple function $Y$, disjoint $A_i$}
$A_i$ disjoint and $\cup A_i=\Omega$, $P(A_i)>0$, $Y=\sum_{i\geq1}c_i I_{A_i}$

$$
E(X|Y) = \sum_{i=1}^\infty \frac{\int_{A_i} X dP}{P(A_i)}I_{A_i}
$$

\subsubsection{Tower property}

If $\mathcal{H}\subset\mathcal{G}$ is a $\sigma-$field, so $\mathcal{H}\subset\mathcal{G}\mathcal{F}$, then
$$
E(X|\mathcal{H})=E\{E(X|\mathcal{G})|\mathcal{H}\} \text{ a.s.}
$$
Let $\mathcal{H}=\{\emptyset, \Omega\}$, then $E(X)=E(E(X|\mathcal{G}))$

\subsubsection{Independence}

[Independent events]
$P(\cap_{i\geq 1} A_i)=\prod_{i\geq 1}A_i$

[Independent collections]
$\mathcal{C}_i\subset \mathcal{F}$ are independent iff events $\{A_i\in \mathcal{C}_i:i\in \mathcal{I}\}$ are independent

[Independent random variables]
R.V are independent iff $\sigma(X_i)$ $\forall$ $i$ are independent 

\subsubsection{Checking independence}
$P(X_1\leq a_1, \cdots, X_n \leq a_n) = P(X_1\leq a_1)\cdots P(X_n\leq a_n)$

If $(X_1, \cdots, X_n)$ has joint pdf $f$ w.r.t product measure $\nu_1\times \cdots \times \nu_n$ iff $f(x_1, \cdots, x_n) = f_1(x_1)\cdots f_n(x_n)$

\subsubsection{Properties}
$E(XY)=E(X)E(Y)$,

$E(X|Y)=E(X)$ $P-$a.s.,

$E(g(X,Y)|Y=y)=E(g(X, y))$ $P_Y$-a.s.,

Any Borel functions are also independent

\subsubsection{Tuple independence}
If $(X, Y_1)$ and $Y_2$ are independent

$E(X|(Y_1, Y_2)) = E(X|Y_1)$ a.s.

$P(A|Y_1, Y_2) = P(A|Y_1)$ a.s. for any $A\in\sigma(X)$

\subsection{Conditional distribution}

\subsubsection{Conditional distribution}

Suppose $X$ is a random $n-$vector on probability space $(\Omega, \mathcal{F}, P)$, and $Y$ is measurable from $(\Omega, \mathcal{F})$ to $(\Lambda, \mathcal{G})$. Then there exists a function $P_{X|Y}(B|y)$ on $\mathcal{B}^n\times \Lambda $ s.t.

(1) $P_{X|Y}(\cdot|y)$ is a probability measure on $(\mathcal{R}^n, \mathcal{B}^n)$ for any fixed $y\in\Lambda$

(2) $P_{X|Y}(B|y)=P[X\in B |Y = y]$ a.s. $P_Y$ for any fixed $B\in \mathcal{B}^n$

\subsubsection{Conditional PDF}
If $(X, Y)$ have PDF $f(x, y)$ w.r.t $\nu\times\lambda$ on $\mathcal{B}^n, \mathcal{B}^m$ and both $\sigma$-finite.
Let $f_Y(y)=\int f(x, y)d\nu (x)$ be marginal PDF of $Y$ w.r.t $\lambda$ and $A=\{y\in\mathcal{R}^m:f_Y(y)>0\}$. Then

(a) For any fixed $y\in A$ the PDF of $P_{X|Y=y}$ w.r.t $\nu$ is given by 
$$
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)}
$$

(b) Furthermore, if $g(x, y)$ is a Borel function on $\mathcal{R}^{n+m}$ and $Eg(X,Y) < \infty$, then
$$
E[g(X, Y)|Y] = \int g(x, Y)f_{X|Y}(x|Y) d\nu(x) \text{ a.s.}
$$

\subsubsection{Joint distribution}
Let $(\Lambda, \mathcal{G}, P_0)$ be probability space. Suppose $Q$ is a function from $\mathcal{B}^n\times \Lambda$ to $\mathcal{R}$ and satisfies

(1) $Q(\cdot, y)$ is a probability measure on $(\mathcal{R}^n, \mathcal{B}^n)$ for any $y\in\Lambda$

(2) $Q(B, \cdot)$ is $\mathcal{G}-$measurable for any $B\in\mathcal{B}^n$.

Then there is a unique probability measure $P$ on $(\mathcal{R}^n\times\Lambda, \sigma(\mathcal{B}^n)\times\mathcal{G})$ s.t. for $B\in\mathcal{B}^n$ and $C\subset\mathcal{G}$

$$
P(B\times C) = \int_C Q(B, y) dP_0(y)
$$

\subsection{Convergence}

\subsubsection{Almost sure convergence} $X_1, X_2, \cdots$ converges almost surely to rvs $X$: $X_n\rightarrow^{\text{a.s.}}X$ if
$$
P\left(\lim_{n\rightarrow\infty} X_n = X\right) = 1
$$

Can be shown by showing $\forall \epsilon > 0, \sum_{i=1}^\infty P(|X_n - X| > \epsilon) < \infty$ via Borel-cantelli lemmas.

\subsubsection{Infinity often} Let $\{A_n\}_{n=1}^\infty$ be an infinite sequencs of events. We say $\{A_n\}$ happens infinitelty often if $\omega$ belongs to following set

$$
\{A_n~i.o.\} = \cap_{n\geq1}\cup_{j\geq n} A_j := {\lim\sup}_{n\rightarrow\infty} A_n
$$

\subsubsection{Borel-Cantelli lemmas}

[First Borel-Cantelli] For a sequence of events $\{A_n\}_{n=1}^\infty$, if $\sum_{n=1}^\infty P(A_n) < \infty$, then $P(A_n ~i.o.)=0$

[Second Borel-Cantelli] For a sequences of pairwisely independent events $\{A_n\}_{n=1}^\infty$, if $\sum_{n=1}^\infty P(A_n)=\infty$, then $P(A_n~i.o.)=1$

\subsubsection{a.s. and First BC} Let $X$ and $X_1, X_2, \cdots$ defined on a common probability space. For a constant $\epsilon > 0$, define the sequences of events $\{A_n(\epsilon)\}_{n=1}^\infty$ to be

$$
A_n(\epsilon) = \{\omega\in\Omega: |X_n(\omega)-X(\omega)|>\epsilon\}
$$

If $\sum_{n=1}^\infty P(\{A_n(\epsilon\})<\infty$ for all $\epsilon > 0$, then $X_n\rightarrow^{\text{a.s.}}X$


\subsubsection{Convergence in $L^p$}
A sequence of $\{X_n\}_{n=1}^\infty$ of rvs converges to a random variable $X$ in the $L^p$ sense for some $p>0$ if $E|X|^p<\infty$ and $E|X_n|^p<\infty$ and 

$$
\lim_{n\rightarrow\infty} E|X_n-X|^p = 0
$$

\subsubsection{Convergence in probability}
A sequence $\{X_n\}_{n=1}^\infty$ of rvs converges to a rando variable $X$ in probability if for all $\epsilon>0$

$$
\lim_{n\rightarrow\infty} P(|X_n-X|>\epsilon) = 0
$$

denoted by $X_n\rightarrow^P X$. Can also be showned by $E(X_n)=X$, $\lim_{n\rightarrow\infty}Var(X_n) = 0$

\subsubsection{Convergence in distribution (weak convergence)}
A sequence $\{X_n\}_{n=1}^\infty$ of rvs converges to a random variable $X$ in distribution/in law/weakly if

$$
\lim_{n\rightarrow} F_n(x) = F(x)
$$

for every $x\in\mathcal{R}$ at which $F$ is continuous, where $F_n, F$ are CDF of $X_n, X$ respectively. Denoted by $X_n\rightarrow^D X$ or $F_n\Rightarrow F$

\subsubsection{Relations between Convergence Modes}

$L^p\Rightarrow L^q\Rightarrow P$, $a.s. \Rightarrow P$, $P \Rightarrow D$.

If $X_n\rightarrow_D C$, then $X_n \rightarrow_P C$. If $X_n\rightarrow_P X$, there exists sub-sequence s.t. $X_{n_j}\rightarrow_{\text{a.s.}}X$.

\subsubsection{Continuous mapping} Let $\{X_n\}_{n=1}^\infty$ be seq of random $k-$vectors and $X$ is random $k-$vector in the same probability space. Let $g: \mathcal{R}^k\rightarrow\mathcal{R}$ be continuous. Then
If $X_n \rightarrow^{\text{*}} X$, then $g(X_n) \rightarrow^{\text{*}}g(X)$, where * is either a.s., $P$ or $D$.

\subsubsection{Convengence properties}

1. Unique in limit: $X=Y$ if $X_n\rightarrow X$ and $Y$ when a.s., $P$, $L^p$. If $F_n\Rightarrow F$ and $G$, then $F(t)=G(t)$ $\forall$ $t$

2. Concatenation: $(X_n, Y_n) \rightarrow (X, Y)$ when $P$ or a.s., $(X_n, Y_n)\rightarrow_D (X, c)$ only for constant.

3. Linearity: $(aX_n+bY_n)\rightarrow aX+bY$ when a.s., $P$, $L^p$ NOT for distribution.

4. Cramér-Wold device: for $k$-random vectors, $X_n\rightarrow_D X$ $\Leftrightarrow$ $c^TX_n\rightarrow_D c^T X$ for every $c\in\mathcal{R}^k$

\subsubsection{Lévy continuity}
$\{X_n\}$ converges in distribution to $X$ iff corresponding characteristic functions $\{\phi_n\}$ converges pointwise to $\phi_X$

\subsubsection{Scheffés theorem} Let $\{f_n\}$ be seq of pdfs on $\mathcal{R}^k$ wrt measure $\nu$. Suppose $\lim_{n\rightarrow\infty} f_n(x)=f(x)$ a.e. $\nu$ and $f(x)$ is pdf wrt $\nu$. Then $\lim_{n\rightarrow\infty} \int |f)n(x) - f(x)|d\nu=0$ and $P_{f_n} \Rightarrow P_f$. Useful for checking convergence in distribution via pdfs.

\subsubsection{Slutsky's theorem}

If $X_n\rightarrow^D X$ and $Y_n \rightarrow^D c$ for a constant $c$. Then
$X_n + Y_n \rightarrow^D X + c$, $X_nY_n \rightarrow^D cX$, $X_n/Y_n \rightarrow^D X/c$ if $c\neq0$

\subsubsection{Skorohod's theorem} If $X_n\rightarrow^D X$, then there are some random vectors $Y, Y_1, Y_2, \cdots$ defined on a common probability space such that $P_{Y_n}=P_{X_n}, n=1, 2, \cdots$, $P_Y=P_X$ and $Y_n\rightarrow^{\text{a.s.}}Y$

\subsubsection{$\delta$-method} Let $X_1, X_2, \cdots, Y$ be rvs, $\{a_n\} > 0$ with $\lim_{n\rightarrow}\infty a_n = \infty$ and $a_n(X_n-c)\rightarrow^D Y$ where $c\in\mathcal{R}$. Let $g$ be a function from $\mathcal{R}$ to $\mathcal{R}$.

If $g(c)$ differentiable at $c$, then
$$
a_n[g(X_n) - g(c)]\rightarrow^D g'(c) Y
$$

Suppose that $g$ has continuous derivatives of order $m > 1$ in a neighbourhood of $c$ s.t. $g^{(j)}(c) = 0$ for all $1\leq j \leq m-1$ and $g^{(m)}(c) \neq 0$. Then

$$
a_n^m [g(X_n)-g(c)]\rightarrow^D \frac{1}{m!}g^{(m)}(c) Y^m
$$

If $X_i, Y$ are $k$-vectors rvs and $c\in\mathcal{R}^k$

$$
a_n[g(X_n)-g(c)]\rightarrow_D [\nabla g(c)]^T Y = N\left(0, g(c)^T \Sigma g(c)\right) \text{ if $Y$ is normal}
$$

\subsection{Stochastic order}

\subsubsection{real numbers}

$\{a_n\}, \{b_n\}$
For a constant $c$ and all $n$
$$
a_n=O(b_n)\Leftrightarrow |a_n| \leq c|b_n|
$$

$$
a_n=o(b_n) \Leftrightarrow \lim_{n\rightarrow\infty}a_n/b_n = 0
$$

\subsubsection{rvs}
$\{X_n\}, \{Y_n\}$

$$
X_n = O_{\text{a.s.}}(Y_n) \Leftrightarrow P\{|X_n|=O(|Y_n|)\}=1
$$

$$
X_n = o_{\text{a.s.}}(Y_n) \Leftrightarrow X_n/Y_n\rightarrow^{\text{a.s.}}0
$$

$\forall~\epsilon>0, \exists C_\epsilon > 0, n_\epsilon \in \mathcal{N} s.t.$
$$
X_n = O_P(Y_n) \Leftrightarrow \sup_{n\geq n_\epsilon} P\left(\{
\omega\in\Omega: |X_n(\omega)\geq C_\epsilon |Y_n(\omega)|
\}\right) < \epsilon
$$
If $X_n = O_P(1)$, $\{X_n\}$ is bounded in probability

$$
X_n = o_P(Y_n) \Leftrightarrow X_n/Y_n \rightarrow^P 0
$$

\subsubsection{Properties}

If $X_n\rightarrow_{\text{a.s.}}X$, then $\{\sup_{n\geq k} |X_n|\}_k$ is $O_p(1)$.

If $X_n\rightarrow_D X$ for a rvs, then $X_n = O_P(1)$ (tightness).

If $E|X_n| = O(a_n)$, then $X_n=O_P(a_n)$; If $E|X_n|=o(a_n)$, then $X_n=o_P(a_n)$

\subsection{Law of Large Numbers (LLN)}

Let $X_1, X_2, \cdots$ be independent rvs.

\subsubsection{Strong LLN} If $X_i$ are identical, let $c:= EX_i$
$$E|X_i|<\infty\Leftrightarrow \frac{1}{n}\sum_{i=1}^n X_i \rightarrow^{\text{a.s.}} c$$

\subsubsection{SLLN, non-identical}

If there is a constant $p\in[1, 2]$ s.t. $\sum_{i=1}^\infty E|X_i|^p/i^p<\infty$, then
$$
\frac{1}{n}\sum_{i=1}^n (X_i-EX_i)\rightarrow^{\text{a.s.}} 0
$$

\subsubsection{Uniform SLLN for iid samples}
Suppose (1) $U(x, \theta)$ is continuous in $\theta$ for any fixed $x$ (2) For each $\theta$, $\mu(\theta)=EU(X, \theta)$ is finite (3) $\Theta$ is compact (4) There exists function $M(x)$ s.t. $EM(X) < \infty$ and $|U(x, \theta)\leq M(x)|$ for all $x, \theta$. Then
$$
P\left\{
    \lim_{n\rightarrow\infty}\sup_{\theta\in\Theta} \left|
        \frac{1}{n}\sum_{i=1}^n U(X_j, \theta)-\mu(\theta)
    \right| = 0
\right\} = 1
$$

\subsubsection{Weak LLN}

If $X_i$ are identical, $\{a_n\}$ exist and take $a_n= E(X_1I_{\{|X_1|\leq n\})} \in [-n, n]$
$$
nP(|X_1| > n) \rightarrow 0 \Leftrightarrow \frac{1}{n} \sum_{i=1}^n X_i - a_n \rightarrow^\mathcal{P} 0
$$

\subsubsection{WLLN, non-identical} If there is a constant $p\in[1, 2]$ s.t. $\lim_{n\rightarrow\infty}\frac{1}{n^p}\sum_{i=1}^n E|X_i|^p=0$, then
$$
\frac{1}{n}\sum_{i=1}^n (X_i-EX_i)\rightarrow^{P} 0
$$

\subsubsection{Weak Convergency}

Seq of probability measures $\nu_n$ converges weakly to $\nu$ if $\int f d\nu_n\rightarrow \int f d\nu$ for every bounded and continous real function $f$.

Suppose $X_n$'s and $X$ are $k$-vectors, $X_n\rightarrow_D X$ is equivalent to any of the conditions below

(1) $E[h(X_n)]\rightarrow E[h(X)]$ for every bounded continuous function $h$ (convergence of probability measures)

(2) $\lim\sup_n P_{X_n}(C)\leq P_X(C)$ for any closed set $C\subset \mathcal{R}^k$

(3) $\lim\inf_n P_{X_n}(O)\geq P_X(O)$ for any open set $O\subset \mathcal{R}^k$

\subsubsection{Central Limit Theorem, Classical iid}

Let $\{X_n\}_{n=1}^\infty$ be seq of iid random $k-$vectors. Suppose $\Sigma=VarX_1<\infty$, then
$$
\frac{1}{\sqrt{n}}\sum_{i=1}^n (X_i-EX_i)\rightarrow^D N(0, \Sigma)
$$

\subsubsection{Lindeberg's CLT for non-identical}

For each $n$, let $\{X_{nj}, j=1, \cdots, k_n\}$ be set of independent rvs. Suppose

(1) $k_n\rightarrow\infty$ as $n\rightarrow\infty$

(2) $0<\sigma_n^2 = Var\left(\sum_{j=1}^{k_n} X_{nj}\right)<\infty, n=1, 2, \cdots$. [Lindeberg's condition]

(3) If for any $\epsilon > 0$, $\frac{1}{\sigma_n^2}\sum_{j=1}^{k_n}E\left\{(X_{nj}-EX_{nj})^2I_{\{|X_{nj}-EX_{nj}|>\epsilon\sigma_n\}}\right\}\rightarrow 0$. Then

$$
\frac{1}{\sigma_n}\sum_{j=1}^{k_n} (X_{nj}-EX_{nj})\rightarrow^D N(0, 1)
$$

\subsubsection{Checking Lindeberg's condition}

Following 2 implied Linderberg's condition

[Lyapunov condition]
$$
\frac{1}{\sigma_n^{2+\delta}}\sum_{j=1}^{k_n} E|X_{nj}-EX_{nj}|^{2+\delta}\rightarrow 0 \text{ for some } \delta > 0
$$

[Uniform boundedness]
If $|X_{nj}|\leq M$ for all $n$ and $j$ and $\sigma_n^2 = \sum_{j=1}^{k_n}Var(X_{nj})\rightarrow \infty$

[Feller's condition]
In general, Lindeberg's condition is not necessary for convergence result. However, if Feller's condition is met then it is sufficient and necessary.

$$
\lim_{n\rightarrow \infty} \max_{j\leq k_n} \frac{Var(X_{nj})}{\sigma_n^2} = 0
$$

\subsubsection{Berry-Esseen Theorem}
There exist a universal constant $C$ such that following holds. Suppose $Y_1, Y_2, \cdots, Y_n$ are iid rvs with $E(Y_i)=0, E(Y_i^2)=\sigma^2>0, E(|Y_i|^3)=\rho<\infty$. Let $F_n$ be CDF of $\frac{\sum_{i=1}^n Y_i}{\sigma\sqrt{n}}$, $\Phi$ be CDF of $N(0,1)$, then
$$
\sup_{y\subset\mathcal{R}}|F_n(y)-\Phi(y)|\leq \frac{C\rho}{\sigma^3\sqrt{n}}
$$