\section{Statistical Estimation}

\subsection{Basics terms}

[Estimator] Estimate $\theta$, is a function of data (is a statistics)

\pline
[Statistical models] A statistical model is often express as 
$$
P\in\mathcal{P}=\{Q: Q \text{ satisfies some condition}\}
$$

\pline
[Population and Sample]
A population is a probability space $(\Omega, \mathcal{F}, P)$, we also refer to $P$ as the population.

A sample is a random element defined on the probability space. The data set is a realization of the sample.

A population $P$ is known iff $P(A)$ is known value for every event $A\in \mathcal{F}$.

\pline
[Parametric family]
A set of probability measures $P_\theta$ on $(\Omega, \mathcal{F})$ indexed by parameter $\theta\in\Theta$ is parametric family iff $\Theta\subset\mathcal{R}^d$.
$\Theta$ is the parameter space, $d$ is its dimensions.

\pline
[Parametric models] Parametric model refers to assumption that the population $P$ is in a parametric family $\mathcal{P}=\{P_\theta:\theta\in\Theta\}$

\pline
[Identifiable] Parametric family is identifiable iff $\forall$ $\theta_1, \theta_2\in\Theta$ and $\theta_1\neq\theta_2\Rightarrow P_{\theta_1}\neq P_{\theta_2}$

\pline
[Dominated by] If $P << \nu$ ($\sigma$-finite measure) $\forall$ $P\in\mathcal{P}$, then $\mathcal{P}$ is dominated by $\nu$.
Also, then $\mathcal{P}$ can be identified by the family of densities $\{\frac{dP}{d\nu}: P\in\mathcal{P}\}$

\subsection{Statistics}

A statistics $T(X)$ is a measurable function of sample $X$

[Sample mean] $\bar{X}=\frac{1}{n}\sum_i X_i$,
$E(\bar{X})=\mu$, $Var(\bar{X})=\sigma^2/n$. If $P\sim N(\mu, \sigma^2), \bar{X}\sim N(\mu, \sigma^2/n)$, If $P\sim E(0, \theta), n\bar{X}\sim \Gamma(n, \theta)$.

[Sample Variance] $S^2=\frac{1}{n-1}\sum_i(X_i-\bar{X})^2$

[Ordered Statistics] $X_(k)$ which is the $k$th smallest value of $X_1, \cdots, X_n$.

$$X_{(n)}=[F(x)]^n, f_{X_{(n)}}=nf(x)[F(x)]^{n-1}$$

$$X_{(1)}=1-[1-F(x)]^n, f_{X_{(1)}}=nf(x)[1-F(x)]^{n-1}$$

[Min, Max] $X_{(1)}, X_{(n)}$

[Empirical variance] $\frac{1}{n}\sum_i (X_i - \bar{X})^2$

[Empirical distribution] $P_n(A)=\frac{1}{n}\#\{i: X_i \in A\}$, $\forall$ $A\in B$

\subsection{Exponential families}
Parametric family $\{P_\theta:\theta\in\Theta\}$ dominated by $\sigma$-finite measure $\nu$ on $(\Omega, \varepsilon$ is called exponential family iff for $\omega\in\Omega$

$$
f_{\theta}(\omega)=\frac{dP_\theta}{d\nu}(\omega) = \exp\left\{[\eta(\theta)]^T T(\omega)-\xi(\theta)\right\}h(\omega)
$$

where $T$ is a random $p$-vector, $\eta$ is a function from $\Omega$ to $\mathcal{R}^p$, h is non-negative Borel function on $(\omega, \varepsilon)$ and 

$$
\xi(\theta) =\log\left\{\int_\Omega \exp\left\{[\eta(\theta)]^T T(\omega)\right\}h(\omega) d\nu(\omega)\right\}
$$

\subsubsection{Canonical form and Natural Exp Families}

Since exp fam representation is not unique, consider $\eta=\eta(\theta)$

$$
f_\eta(\omega) = \exp\left\{\eta^T T(\omega)-\mathcal{C}(\eta)\right\}h(\omega)
$$
$$\mathcal{C}(\eta)=\log\left\{\int_\Omega \exp\left\{\eta^T T(\omega)\right\}h(\omega)d\nu(\omega)\right\}$$

$\eta$ is called natural parameter and natural parameter space $\Xi=\{\eta(\theta):\theta\in\Theta\}\subset\mathcal{R}^p$. Full rank if $\Xi$ contains open set in $\mathcal{R}^p$

\subsubsection{Joint Exp Fam}
Suppose $X_i\sim f_i$ independently with $f_i$ Exp Fam, then joint distribution $X_1, \cdots, X_n$ is also Exp Fam.

\subsubsection{Showing non Exp Fam}
For an exp fam $P_\theta$, there is nonzero measure $\lambda$ s.t. $\frac{dP_\theta}{d\lambda}(\omega)>0$ $\lambda$-a.e. and for all $\theta$.

Consider $f=\frac{dP_\theta}{d\lambda}I_{(t, \infty)}(x)$,
$\int fd\lambda=0, f\geq 0\Rightarrow f=0$. Since $\frac{dP_\theta}{d\lambda}>0$ (assume), then $I_{(t, \infty)}(x)=0\Rightarrow v([t, \infty))=0$. Since $t$ is arbitary, consider $v(\mathcal{R})=0$ (contradiction)

\subsubsection{Separate statistics $T$}

Let $T=(Y, U)$ and $\eta=(\nu, \varphi)$ where $Y$ and $\nu$ have same dimension. Then $Y$ has PDF

$$
f_\eta(y)=\exp\left\{\nu^T y - \mathcal{C}(\eta)\right\}
$$

w.r.t $\sigma$-finite measure depending on $\varphi$.
If $T$ has a PDF in NEF, the conditional distirbution of $Y$ given $U=u$ has PDF (w.r.t $\sigma$-finite measure depending on $u$)

$$
f_{\nu, u}(y) = \exp\left\{\nu^T y - \mathcal{C}_u(\nu)\right\}
$$

which is in a NEF indexed by $\nu$

\subsubsection{MGF of NEFs} If $\eta_0$ is an interior point on natural parameter space, then MGF $\phi_{\eta_0}(t)$ of $T$ (with $P=P_{\eta_0}$ is finite in neighborhood of $t=0$ and is given by

$$
\psi_{\eta_0}(t) = \exp\left\{\mathcal{C}(\eta_0+t)-\mathcal{C}(\eta_0)\right\}
$$

Let $A(\theta)=\mathcal{C}(\eta_0(\theta))$, $\frac{dA(\theta)}{d\theta}=\frac{d\mathcal{C}(\eta_0(\theta))}{d\eta_0(\theta)}\cdot\frac{d\eta_0(\theta)}{d\theta}$

$$
E_{\eta_0} T = \frac{d\psi_{\eta_0}}{dt}|_{t=0} = \frac{d\mathcal{C}}{d\eta_0}
=\frac{A'(\theta)}{\eta_0'(\theta)}$$

$$
E_{\eta_0}T^2 = \mathcal{C}''(\eta_0) + \mathcal{C}'(\eta_0)^2
$$

$$
Var(T) = \mathcal{C}''(\eta_0) = \frac{A''(\theta)}{[\eta_0(\theta)]^2} - \frac{\eta_0(\theta)''A'(\theta)}{[\eta_0(\theta)']^3}
$$

\subsubsection{Differential identities of NEFs}
For a Borel function $g$, let $\Xi_g$ be set of values of $\eta$ such that

$$
\int |g(\omega)|\exp\left\{\eta^T T(\omega)-\mathcal{C}(\eta)\right\}h(\omega) d\nu(\omega) < \infty
$$

Define $G$ on $\Xi_g$ by

$$
G(\eta) := \int g(\omega) \exp\left\{\eta^T T(\omega)-\mathcal{C}(\eta)\right\}h(\omega) d\nu(\omega)
$$

Then for $\eta$ in interior of $\Xi_g$

(1) $G$ is continuous and has continuous derivatives of all orders.

(2) These derivatives can be computed by differentiation under the integral sign.

$$
\frac{dG(\eta)}{d\eta} = E_\eta \left[g(\omega) \left(T(\omega) - \frac{\partial}{\partial\eta}\xi(\eta)\right)\right]
$$

\subsection{Data Reduction}

\subsubsection{Sufficiency}
Let $X$ be a sample from an unknown population $P\in\mathcal{P}$. Statistics $T(X)$ is sufficient for $P\in\mathcal{P}$ iff $P_X(x|Y)$ is known and does not depend on $P$.
If $\mathcal{P}$ is parametric family, we can also say $T(X)$ is sufficient for $\theta$. Suppose $T$ is sufficient for $\mathcal{P}_0$, $\mathcal{P}_0\subset \mathcal{P}\subset \mathcal{P}_1$. Then $T(X)$ is sufficient for $\mathcal{P}_0$ but not ncessarily $\mathcal{P}_1$.

$$
P(X=x|T=t) \text{ does not depend on } \theta
$$

\subsubsection{Factorization theorem}
$T(X)$ is sufficient for $P\in\mathcal{P}$ iff there are non-negative Borel functions

(1) $h(x)$ does not depend on $P$

(2) $g_P(t)$ which depends on $P$

s.t.

$$
\frac{dP}{d\nu}(x) = g_P(T(x))h(x)
$$

\subsubsection{Minimal sufficiency}

Let $T$ be a sufficient statistics for $P\in\mathcal{P}$. $T$ is called minimal sufficient statistics iff for any other statistics $S$ sufficient for $P\in\mathcal{P}$, there is a measurable function $\psi$ s.t. $T=\psi(S)$ $\mathcal{P}$-a.s.

\subsubsection{Min Suff - Method 1}

[Theorem A]
Suppose $\mathcal{P}_0\subset\mathcal{P}$ and $\mathcal{P}_0$-a.s. implies $\mathcal{P}$-a.s.
If $T$ is sufficient for $P\in\mathcal{P}$ and minimal sufficient for $P\in\mathcal{P}_0$, then $T$ is minimal sufficient for $P\in\mathcal{P}$

[Theorem B]
Suppose $\mathcal{P}$ contains PDFs $f_0, f_1, \cdots$ w.r.t a $\sigma$-finite measure.

(1) Define $f_\infty(x)=\sum_{i=0}^\infty c_if_i(x)$, $T_i(x)=f_i(x)/f_\infty(x)$, then $T(X)=(T_0(X), T_1(X), \cdots)$ is minimal sufficient for $\mathcal{P}$. Where $c_i>0, \sum_{i=0}^\infty c_i=1, f_{\infty}(x)>0$.

(2) If $\{x:f_i(x)>0\}\subset \{x: f_0(x) > 0\}$ for all $i$, then $T(X)=(f_1(x)/f_0(x), f_2(x)/f_0(x), \cdots$ is minimal sufficient for $\mathcal{P}$

\subsubsection{Min Suff - Method 2}

[Theorem C]
Suppose $\mathcal{P}$ contains PDFs $f_P$ w.r.t. $\sigma$-finite measure $\nu$. If

(a) $T(X)$ is a sufficient statistics, and

(b) There is a measurable function $\phi$ s.t. for any possible values $x, y$ of $X$, or $x, y\in\{x:h(x)>0\}$ for NEF.

$$
f_P(x) = f_P(y)\phi(x, y) \forall P\in\mathcal{P} \Rightarrow T(x)=T(y)
$$

Then $T(X)$ is minimal sufficient for $\mathcal{P}$

\subsubsection{Special min suff result for NEF}

If there exists $\Theta_0=\{\theta_0, \theta_1, \cdots, \theta_p\}\subset \Theta$ s.t. vectors $\eta_i=\eta(\theta_i)-\eta(\theta_0), i \in [1, p]$ are linearly independent in $\mathcal{R}^p$, then $T$ is also minimal sufficient.
Check $det([\eta_1, \cdots, \eta_p])$ is non-zero OR $\Xi = \{\eta(\theta):\theta\in\Theta\}$  contains $(p+1)$ points that do not lie on the same hyperplane OR $\Xi$ is full rank.

\subsubsection{Completeness}

[Ancillary statistics]
A statistics $V(X)$ is ancillary for $\mathcal{P}$ if its distribution does not depend on population $P\in\mathcal{P}$

[First-order ancillary] if $E_P[V(X)]$ does not depend on $P\in\mathcal{P}$

[Completeness] Statistics $T(X)$ is complete for $P\in\mathcal{P}$ iff for any Borel function $f$, $E_P f(T)=0$ for all $P\in\mathcal{P}$ implies $f(T)=0$ $\mathcal{P}$-a.s.
$T$ is boundedly complete iff statements holds for bounded Borel functions $f$.

\subsubsection{Completeness + Sufficiency $\Rightarrow$ Minimal Sufficiency}

Suppose $X$ is a sample from unknown $P\in\mathcal{P}$, and suppose a minmal sufficient statistics exists.
If a statistics $U$ is sufficient and boundedly complete, then $U$ is minimal sufficient

\subsubsection{Complete sufficient statistics for NEF}

If $\mathcal{P}$ is NEF of full rank then $T(X)$ is complete and sufficient for $\eta\in\Xi$

\subsection{Basu's theorem}

Let $V$ and $T$ be two statistics of $X$ from a population $P\in\mathcal{P}$. If $V$ is ancillary and $T$ is boundedly complete and sufficient for $P\in\mathcal{P}$, then $V$ and $T$ are independent w.r.t any $P\in\mathcal{P}$ 